% --------------------------------------------------------------
% This is all preamble stuff that you don't have to worry about.
% Head down to where it says "Start here"
% --------------------------------------------------------------

\documentclass[12pt]{article}

\usepackage{fontspec}
\usepackage{xeCJK}
\usepackage[margin=1in]{geometry}
\usepackage{amsmath,amsthm,amssymb}
\usepackage{graphicx}

\setCJKmainfont{LiHei Pro}
\XeTeXlinebreaklocale zh
\XeTeXlinebreakskip = 0pt plus 1pt

% ------ For pasting codes ------
\usepackage{listings}
\usepackage{color}

\definecolor{dkgreen}{rgb}{0,0.6,0}
\definecolor{gray}{rgb}{0.5,0.5,0.5}
\definecolor{mauve}{rgb}{0.58,0,0.82}

\lstset{frame=tb,
  language=C,
  aboveskip=3mm,
  belowskip=3mm,
  showstringspaces=false,
  columns=flexible,
  basicstyle={\small\ttfamily},
  numbers=none,
  numberstyle=\tiny\color{gray},
  keywordstyle=\color{blue},
  commentstyle=\color{dkgreen},
  stringstyle=\color{mauve},
  breaklines=true,
  breakatwhitespace=true,
  tabsize=3
}
% -----------------------------------

\begin{document}

% --------------------------------------------------------------
%                         Start here
% --------------------------------------------------------------

\title{Tutorial: Kernel Methods}
\author{Chih-Wei Chang B00201037\\
Chung-Yen Hung B00201015}
\maketitle

\begin{abstract}
  Here comes the Abstract
\end{abstract}

\section{Introduction to Kerenl Method}
\subsection{What Is Kernel and Kernel Trick}
\subsection{Why Use Kernel and How Kernel Can Be Used}

\section{Common Kernel}
\subsection{Conditions To Be A Valid Kerenl}
\subsection{Hilber Space and RKHS}
\subsection{Polynomial Kernel}
The polynomial kernel is define as 
$$ 
K_{\Phi_{d}}(x, y) = {(< x,y > + \alpha)}^{d}
$$\par
As a kernel, K corresponds to an inner product in a feature space based on some mapping:
$$ 
K_{\Phi_{d}}(x, y) = <\Phi_{d}(x), \Phi_{d}(y)>
$$\par
Let $ d=2 $, so we get the special case of the quadratic kernel

$$
K(x, y) = {(\sum_{i=1}^{n} x_{i}y_{i} + \alpha)}^{2} = \sum_{i=1}^{n}(x_{i}^{2})(y_{i}^{2}) + \sum_{i=2}^{n}\sum_{j=1}^{i-1}(\sqrt{2}x_{i}x_{j})(\sqrt{2}y_{i}y_{j}) + \sum_{i=1}^{n}(\sqrt{2c}x_{i})(\sqrt{2c}y_{i})
$$\par

From this it follows that the feature map is given by:
$$
\Phi_{2}(x) = <x_{n}^{2},\dots,x_{1}^{2},\sqrt{2}x_{n}x_{n-1},\dots,\sqrt{2}x_{n}x_{1},\sqrt{2}x_{n-1}x_{n-2},\dots,\sqrt{2c}x_{n},\dots,\sqrt{2c}x_{1},c>
$$


\subsection{Gaussian Kernel}

\section{Kernel Machines}
\subsection{Kernel PCA}
\subsection{Kernel SVM}
\subsection{Kerenl Ridge Regression}
\subsection{Kernel Logistic Regression}

\section{Output Kernel}
\subsection{Kernel in Output Space}
\subsection{PLST and CPLST}
\subsection{Other Possible Output Kernel Techniques}

\section{Conclusion}
Here comes the Conclusion


\bibliographystyle{plain}
\bibliography{ref}


% --------------------------------------------------------------
%     You don't have to mess with anything below this line.
% --------------------------------------------------------------

\end{document}
